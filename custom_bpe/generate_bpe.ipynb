{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "chars_punctuation = string.punctuation\n",
    "chars_whitespace = string.whitespace\n",
    "chars_alphanumeric = string.ascii_letters + string.digits\n",
    "\n",
    "\n",
    "def convert_to_ascii(utf8_string):\n",
    "    try:\n",
    "        return utf8_string.encode('ascii', 'ignore').decode('ascii')\n",
    "    except UnicodeEncodeError as e:\n",
    "        print(f\"Conversion from UTF-8 to ASCII failed. Error: {e}\")\n",
    "        print(utf8_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the files in the data directory to ASCII\n",
    "import os\n",
    "\n",
    "filedir_utf8 = \"data_utf8\"\n",
    "filedir_ascii = \"data_ascii\"\n",
    "# Load all the files in the directory\n",
    "files = os.listdir(filedir_utf8)\n",
    "\n",
    "for file in files:\n",
    "    with open(filedir_utf8 + \"/\" + file, \"r\", encoding=\"utf-8\") as f:\n",
    "        with open(filedir_ascii + \"/\" + file, \"w\", encoding=\"ascii\") as f_new:\n",
    "            for line in f:\n",
    "                # line = normalizer.normalize_str(line)\n",
    "                f_new.write(convert_to_ascii(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all punctuation with a space\n",
    "# Convert all types of whitespace to a single space\n",
    "import re\n",
    "\n",
    "filedir_ascii = \"data_ascii\"\n",
    "filedir_procd = \"data_procd\"\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    with open(filedir_ascii + \"/\" + file, \"r\", encoding=\"ascii\") as f:\n",
    "        with open(filedir_procd + \"/\" + file, \"w\", encoding=\"ascii\") as f_new:\n",
    "            for line in f:\n",
    "                for char in chars_punctuation:\n",
    "                    line = line.replace(char, \" \")\n",
    "                for char in chars_whitespace:\n",
    "                    line = line.replace(char, \" \")\n",
    "                \n",
    "                # Shrink multiple spaces to a single space\n",
    "                line = re.sub(' +', ' ', line)\n",
    "\n",
    "                f_new.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: wikipedia_1M_0.txt\n",
      "Processing file: wikipedia_1M_1.txt\n",
      "Processing file: wikipedia_1M_2.txt\n",
      "Processing file: wikipedia_1M_3.txt\n",
      "Processing file: wikipedia_1M_4.txt\n",
      "Processing file: wikipedia_1M_5.txt\n",
      "Processing file: wikipedia_1M_6.txt\n",
      "Processing file: wikipedia_1M_7.txt\n",
      "Processing file: wikipedia_1M_8.txt\n",
      "Processing file: wikipedia_1M_9.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Convert in to a list of words, for all lines in the file\n",
    "filedir_procd = \"data_procd\"\n",
    "\n",
    "# for file in files:\n",
    "word_counter = Counter()\n",
    "for file in files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    with open(filedir_procd + \"/\" + file, \"r\", encoding=\"ascii\") as f:\n",
    "        for line in f:\n",
    "            word_counter.update(line.split())\n",
    "word_counts_dict = dict(word_counter)\n",
    "\n",
    "import json\n",
    "\n",
    "word_counts_dict = dict(word_counter.most_common())\n",
    "output_json_path = 'word_counts.json'\n",
    "with open(output_json_path, 'w', encoding='ascii') as json_file:\n",
    "    json.dump(word_counts_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_counter = word_counter.most_common()\n",
    "total_words = sum(word_counter.values())\n",
    "cumulative_percentage = 0\n",
    "selected_words = []\n",
    "for word, count in sorted_word_counter:\n",
    "    word_percentage = count / total_words\n",
    "    cumulative_percentage += word_percentage\n",
    "    selected_words.append((word, count))\n",
    "    \n",
    "    if cumulative_percentage >= 0.90:\n",
    "        break\n",
    "\n",
    "print(f\"Total words: {total_words}\")\n",
    "print(f\"Selected words: {len(selected_words)}\")\n",
    "\n",
    "output_selected_json_path = 'word_counts_90.json'\n",
    "with open('word_counts_90.json', 'w', encoding='ascii') as json_file:\n",
    "    json.dump(selected_words, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load the selected words\n",
    "with open('word_counts_90.json', 'r', encoding='ascii') as json_file:\n",
    "    selected_words = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99th Top pair: ('h', 'e '), count: 581895\n",
      "199th Top pair: ('a', 'b'), count: 271495\n",
      "299th Top pair: ('em', 'ber '), count: 166778\n",
      "399th Top pair: ('b', 'ec'), count: 124395\n",
      "499th Top pair: ('be', 'tw'), count: 93922\n",
      "599th Top pair: ('m', 'em'), count: 78413\n",
      "699th Top pair: ('devel', 'op'), count: 65873\n",
      "799th Top pair: ('albu', 'm '), count: 55446\n",
      "899th Top pair: ('le', 'ague '), count: 47723\n",
      "999th Top pair: ('v', 'ir'), count: 41925\n",
      "1099th Top pair: ('n', 'see '), count: 38554\n",
      "1199th Top pair: ('200', '8 '), count: 34466\n",
      "1299th Top pair: ('g', 'l'), count: 31241\n",
      "1399th Top pair: ('gh', 'ts '), count: 28737\n",
      "1499th Top pair: ('to', 'tal '), count: 26477\n",
      "1599th Top pair: ('c', 'ase '), count: 24260\n",
      "1699th Top pair: ('ous', 'ly '), count: 22282\n",
      "1799th Top pair: ('201', '7 '), count: 20895\n",
      "1899th Top pair: ('d', 'augh'), count: 19612\n",
      "1999th Top pair: ('gr', 'and '), count: 18245\n",
      "2099th Top pair: ('h', 'ill '), count: 17050\n",
      "2199th Top pair: ('ma', 'y'), count: 16103\n",
      "2299th Top pair: ('t', 'ting '), count: 15249\n",
      "2399th Top pair: ('d', 'ary '), count: 14389\n",
      "2499th Top pair: ('town', 'ship '), count: 13506\n",
      "2599th Top pair: ('fic', 'tion '), count: 12762\n",
      "2699th Top pair: ('mon', 'ey '), count: 12187\n",
      "2799th Top pair: ('di', 'vi'), count: 11638\n",
      "2899th Top pair: ('cu', 'st'), count: 11109\n",
      "2999th Top pair: ('ge', 'org'), count: 10688\n",
      "3099th Top pair: ('5', 'th '), count: 10220\n",
      "3199th Top pair: ('3', '6 '), count: 9777\n",
      "3299th Top pair: ('p', 'sy'), count: 9358\n",
      "3399th Top pair: ('no', 'tes '), count: 8989\n",
      "3499th Top pair: ('scot', 't '), count: 8613\n",
      "3599th Top pair: ('succ', 'e'), count: 8319\n",
      "3699th Top pair: ('consist', 's '), count: 7963\n",
      "3799th Top pair: ('gu', 'ard '), count: 7662\n",
      "3899th Top pair: ('194', '9 '), count: 7423\n",
      "3999th Top pair: ('appro', 'ach '), count: 7209\n",
      "4099th Top pair: ('f', 'f '), count: 6984\n",
      "4199th Top pair: ('dynast', 'y '), count: 6741\n",
      "4299th Top pair: ('rec', 'i'), count: 6513\n",
      "4399th Top pair: ('r', 'am'), count: 6326\n",
      "4499th Top pair: ('gu', 'est '), count: 6142\n",
      "4599th Top pair: ('shar', 'ed '), count: 5950\n",
      "4699th Top pair: ('g', 'ar '), count: 5770\n",
      "4799th Top pair: ('beli', 'eve '), count: 5588\n",
      "4899th Top pair: ('ob', 'jects '), count: 5449\n",
      "4999th Top pair: ('h', 'tt'), count: 5269\n",
      "5099th Top pair: ('vis', 'it'), count: 5132\n",
      "5199th Top pair: ('essi', 'ons '), count: 4968\n",
      "5299th Top pair: ('plan', 'et '), count: 4853\n",
      "5399th Top pair: ('com', 'pris'), count: 4736\n",
      "5499th Top pair: ('colle', 'ges '), count: 4619\n",
      "5599th Top pair: ('j', 'en'), count: 4463\n",
      "5699th Top pair: ('lincol', 'n '), count: 4341\n",
      "5799th Top pair: ('or', 'eg'), count: 4222\n",
      "5899th Top pair: ('y', 'le '), count: 4097\n",
      "5999th Top pair: ('deliver', 'ed '), count: 3986\n",
      "6099th Top pair: ('voc', 'al '), count: 3892\n",
      "6199th Top pair: ('responsi', 'bility '), count: 3790\n",
      "6299th Top pair: ('6', '2 '), count: 3707\n",
      "6399th Top pair: ('val', 'u'), count: 3626\n",
      "6499th Top pair: ('p', 'airs '), count: 3560\n",
      "6599th Top pair: ('cover', 'age '), count: 3472\n",
      "6699th Top pair: ('pr', 'us'), count: 3394\n",
      "6799th Top pair: ('wid', 'th '), count: 3308\n",
      "6899th Top pair: ('ti', 'le '), count: 3225\n",
      "6999th Top pair: ('lo', 'sses '), count: 3156\n",
      "7099th Top pair: ('autom', 'atic '), count: 3068\n",
      "7199th Top pair: ('8', '7 '), count: 2990\n",
      "7299th Top pair: ('roosevel', 't '), count: 2934\n",
      "7399th Top pair: ('me', 'et'), count: 2873\n",
      "7499th Top pair: ('spea', 'kers '), count: 2816\n",
      "7599th Top pair: ('n', 'plot '), count: 2757\n",
      "7699th Top pair: ('af', 'fair '), count: 2707\n",
      "7799th Top pair: ('dan', 'cing '), count: 2652\n",
      "7899th Top pair: ('gr', 'ass '), count: 2606\n",
      "7999th Top pair: ('glo', 'be '), count: 2565\n",
      "8099th Top pair: ('fl', 'ed '), count: 2514\n",
      "8199th Top pair: ('guit', 'ars '), count: 2452\n",
      "8299th Top pair: ('har', 'vey '), count: 2396\n",
      "8399th Top pair: ('dol', 'lars '), count: 2347\n",
      "8499th Top pair: ('out', 'door '), count: 2299\n",
      "8599th Top pair: ('somer', 'set '), count: 2252\n",
      "8699th Top pair: ('ex', 'port '), count: 2215\n",
      "8799th Top pair: ('ser', 'bian '), count: 2165\n",
      "8899th Top pair: ('mar', 'ry '), count: 2119\n",
      "8999th Top pair: ('hel', 'ps '), count: 2075\n",
      "9099th Top pair: ('b', 'ones '), count: 2039\n",
      "9199th Top pair: ('j', 'ury '), count: 2009\n",
      "9299th Top pair: ('chor', 'us '), count: 1971\n",
      "9399th Top pair: ('sust', 'ain'), count: 1936\n",
      "9499th Top pair: ('vir', 'tually '), count: 1906\n",
      "9599th Top pair: ('proce', 'du'), count: 1872\n",
      "9699th Top pair: ('pr', 'en'), count: 1844\n",
      "9799th Top pair: ('no', 'se '), count: 1814\n",
      "9899th Top pair: ('de', 'ad'), count: 1781\n",
      "9999th Top pair: ('davi', 'es '), count: 1748\n"
     ]
    }
   ],
   "source": [
    "words_dict = dict(selected_words)\n",
    "words_dict = {key + ' ': value for key, value in words_dict.items()}\n",
    "words_lst = list(words_dict.keys())\n",
    "\n",
    "bytepairs = []\n",
    "for w in words_lst:\n",
    "    word_bps = []\n",
    "    for c in w:\n",
    "        word_bps.append(c)\n",
    "    bytepairs.append(word_bps)\n",
    "\n",
    "bp_filename = 'bytepairs.txt'\n",
    "with open(bp_filename, 'w', encoding='ascii') as f:\n",
    "    pass\n",
    "    \n",
    "output_bps = []\n",
    "output_buffer = []\n",
    "top_pair = None\n",
    "\n",
    "for n in range(10000):\n",
    "    counts = dict()\n",
    "    for j in range(len(bytepairs)):\n",
    "        word = bytepairs[j]\n",
    "        original_word = word\n",
    "        i=0\n",
    "        while i < len(word) - 1:\n",
    "            pair = (word[i], word[i+1])\n",
    "            \n",
    "            # If we have a top pair, we need to stick it together the rebulid the word\n",
    "            if pair == top_pair:\n",
    "                new_bytepairs = []\n",
    "                k=0\n",
    "                while k < len(word):\n",
    "                    if k == len(word) - 1:\n",
    "                        new_bytepairs.append(word[k])\n",
    "\n",
    "                    elif (word[k], word[k+1]) == top_pair:\n",
    "                        new_bytepairs.append(word[k] + word[k+1])\n",
    "                        k += 1\n",
    "                        \n",
    "                    else:\n",
    "                        new_bytepairs.append(word[k])\n",
    "                    k += 1\n",
    "\n",
    "                word = new_bytepairs\n",
    "                bytepairs[j] = word # Replace the word char pairing with the new pairing\n",
    "\n",
    "\n",
    "                # Increment the prev word index if we have stuck together a pair\n",
    "\n",
    "                # Skip to the next word if this is now the end of the word\n",
    "                # I.e. if len is now equal to i, given that we have incremented i and the word is now shorter\n",
    "                if i == len(word) -1:\n",
    "                    continue\n",
    "                pair = (word[i], word[i+1])\n",
    "                i += 1\n",
    "\n",
    "            if pair in counts:\n",
    "                counts[pair] += words_dict[''.join(word)]\n",
    "            else:\n",
    "                counts[pair] = words_dict[''.join(word)]\n",
    "\n",
    "            i+=1\n",
    "\n",
    "    # Sort the counts\n",
    "    counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract and append the top pair\n",
    "    top_pair = counts[0][0]\n",
    "    output_bps.append(top_pair[0] + top_pair[1])\n",
    "    output_buffer.append(top_pair[0] + top_pair[1])\n",
    "    if n % 100 == 99:\n",
    "        print(f\"{n+1}th Top pair: {top_pair}, count: {counts[0][1]}\")\n",
    "        # Append the output buffer to file\n",
    "        with open(bp_filename, 'a', encoding='ascii') as f:\n",
    "            for bp in output_buffer:\n",
    "                f.write(bp + '\\n')\n",
    "        output_buffer = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
